{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import qmlhep\n",
    "from os.path import join\n",
    "from qmlhep.config import  others_path\n",
    "from qmlhep.data_handling.dataset import ParticlePhysics\n",
    "import seaborn as sns\n",
    "\n",
    "# Ignore warnings, FutureWarnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset size reduction\n",
    "\n",
    "This notebook is used to train a logistic regression model using the *KMeans* reduced dataset, a dataset reduced using random undersampling, and the original dataset. \n",
    "\n",
    "This notebook is also used to plot the figures that appear in the article.\n",
    "\n",
    "Author: Maria Gabriela Jord√£o Oliveira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the book - to get the features\n",
    "book={}\n",
    "with open(join(others_path, 'SBS.pkl'), 'rb') as f:\n",
    "    book=pickle.load(f)\n",
    "\n",
    "# load data\n",
    "train_data = ParticlePhysics(category='train', standardization='ML', random_seed=42).all_data_Dataframe()\n",
    "train_data.drop(columns=['name'], inplace=True)\n",
    "\n",
    "validation_data = ParticlePhysics(category='validation', standardization='ML', random_seed=42).all_data_Dataframe()\n",
    "validation_data.drop(columns=['name'], inplace=True)\n",
    "\n",
    "\n",
    "# Plot details\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 22\n",
    "LEGEND_SIZE = 14\n",
    "TICK_SIZE = 16\n",
    "sns.set(font_scale=50)  # Data ticks\n",
    "\n",
    "\n",
    "plt.rc(\"font\", size=SMALL_SIZE)  # controls default text sizes\n",
    "plt.rc(\"axes\", titlesize=SMALL_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=LEGEND_SIZE)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original dataset - comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.01])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# to save auc values\n",
    "aucs_original=[]\n",
    "for n_features in [1,2,3,4,5]:\n",
    "\n",
    "    features=book[n_features]\n",
    "    X_train, Y_train, W_train = train_data[features], train_data['label'], train_data['weights']\n",
    "    X_val, Y_val, W_val = validation_data[features], validation_data['label'], validation_data['weights']\n",
    "\n",
    "    \n",
    "    W_train[Y_train == 0] =( W_train[Y_train == 0] / W_train[Y_train == 0].sum())*W_train.shape[0]/2\n",
    "    W_train[Y_train == 1] =( W_train[Y_train == 1] / W_train[Y_train == 1].sum())*W_train.shape[0]/2\n",
    "    W_val[Y_val == 0] =( W_val[Y_val == 0] / W_val[Y_val == 0].sum())*W_val.shape[0]/2\n",
    "    W_val[Y_val == 1] =( W_val[Y_val == 1] / W_val[Y_val == 1].sum())*W_val.shape[0]/2\n",
    "\n",
    "    # Logistic Regression\n",
    "    clf = LogisticRegression(n_jobs=-1)\n",
    "   \n",
    "    clf.fit(X_train,\n",
    "            Y_train, \n",
    "            sample_weight= W_train)\n",
    "\n",
    "    Y_pred = clf.predict_proba(X_val)\n",
    "    y_scores = Y_pred[:, 1]\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(Y_val, y_scores,sample_weight=W_val)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs_original.append(roc_auc)\n",
    "\n",
    "    plt.plot(fpr, tpr, label=f'#Features = {n_features}, AUC = {round(roc_auc,4)}')\n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_curve_lr_full_dataset_oficial.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random undersampling - comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for n_datapoints in [50,250,500,2500]:\n",
    "    aucs = []\n",
    "    error = []\n",
    "    for n_features in [1,2,3,4,5]:\n",
    "        aucs_random_sampling=[]\n",
    "\n",
    "        # r is the random seed for the r th iteration\n",
    "        for r in range(10):\n",
    "            features=book[n_features]\n",
    "\n",
    "            datat = ParticlePhysics(\n",
    "            category=\"train\", standardization=\"ML\", random_seed=r, n_datapoints=2*n_datapoints).all_data_Dataframe()\n",
    "            datat.drop(columns=[\"name\"], inplace=True)\n",
    "\n",
    "            X_resampled, Y_resampled, W_resampled = datat[features], datat[\"label\"], datat[\"weights\"]\n",
    "\n",
    "            datav = ParticlePhysics(\n",
    "            category=\"validation\", standardization=\"ML\", random_seed=r, n_datapoints=2*n_datapoints).all_data_Dataframe()\n",
    "            datav.drop(columns=[\"name\"], inplace=True)\n",
    "\n",
    "            X_resampledV, Y_resampledV, W_resampledV = datav[features], datav[\"label\"], datav[\"weights\"]\n",
    "\n",
    "            # Renormalize weights\n",
    "            W_resampled[Y_resampled == 0] = ( W_resampled[Y_resampled == 0] / W_resampled[Y_resampled == 0].sum())*W_resampled.shape[0]/2\n",
    "            W_resampled[Y_resampled == 1] = ( W_resampled[Y_resampled == 1] / W_resampled[Y_resampled == 1].sum())*W_resampled.shape[0]/2\n",
    "            W_resampledV[Y_resampledV == 0] = ( W_resampledV[Y_resampledV == 0] / W_resampledV[Y_resampledV == 0].sum())*W_resampledV.shape[0]/2\n",
    "            W_resampledV[Y_resampledV == 1] = ( W_resampledV[Y_resampledV == 1] / W_resampledV[Y_resampledV == 1].sum())*W_resampledV.shape[0]/2\n",
    "\n",
    "            # Logistic Regression\n",
    "            clf = LogisticRegression()\n",
    "\n",
    "            clf.fit(X_resampled,\n",
    "                    Y_resampled,\n",
    "                    sample_weight= W_resampled)\n",
    "\n",
    "\n",
    "            Y_pred = clf.predict_proba(X_resampledV)\n",
    "            y_scores = Y_pred[:, 1]\n",
    "\n",
    "            fpr, tpr, _ = roc_curve(Y_resampledV, y_scores,sample_weight=W_resampledV)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aucs_random_sampling.append(roc_auc)\n",
    "    \n",
    "        aucs.append(np.mean(aucs_random_sampling))\n",
    "        error.append(np.std(aucs_random_sampling))\n",
    "    plt.errorbar([1,2, 3, 4, 5],aucs, yerr=error, label=f'{2*n_datapoints} datapoints')\n",
    "plt.plot([1,2, 3, 4, 5], aucs_original, label='Origal dataset', linestyle='--')\n",
    "\n",
    "plt.grid(True)    \n",
    "plt.xlabel('Number of features', fontsize = MEDIUM_SIZE)\n",
    "plt.ylabel('AUC', fontsize = MEDIUM_SIZE)\n",
    "plt.xticks(fontsize = TICK_SIZE)\n",
    "plt.yticks(fontsize = TICK_SIZE)\n",
    "plt.legend(fontsize = MEDIUM_SIZE)\n",
    "plt.savefig(\"plot_roc_undersampling.pdf\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans reduction - train dataset \n",
    "Random undersampling on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(others_path,'kmeans_dataset_train.pkl'),'rb') as f:\n",
    "    samples=pickle.load(f)\n",
    "samples=pd.DataFrame.from_dict(samples)\n",
    "plt.figure(figsize=(10,10))\n",
    "for centrus in [50,250,500,2500]:\n",
    "    aucs_random_val = []\n",
    "    error_random_val = []\n",
    "    \n",
    "    for features in [1,2,3,4,5]:\n",
    "        # choose the number of centrus\n",
    "        samples_k = samples['#Clusters']== centrus\n",
    "        samples_k = samples[samples_k]\n",
    "        \n",
    "        features=book[features]\n",
    "\n",
    "        # Choose the features\n",
    "        X_resampled, Y_resampled, W_resampled = pd.DataFrame(samples_k['X_train'].iloc[0])[features], samples_k['Y_train'].iloc[0], samples_k['W_train1'].iloc[0]\n",
    "        \n",
    "        aucs = []\n",
    "        # r is the random seed of r th iteration\n",
    "        for r in range(10):\n",
    "            datav = ParticlePhysics(\n",
    "            category =\"validation\", standardization=\"ML\", random_seed=r, n_datapoints=2*centrus).all_data_Dataframe()\n",
    "            datav.drop(columns=[\"name\"], inplace=True)\n",
    "\n",
    "            X_resampledV, Y_resampledV, W_resampledV = datav[features], datav[\"label\"], datav[\"weights\"]\n",
    "\n",
    "            # Renormalize weights\n",
    "            W_resampledV[Y_resampledV == 0] = ( W_resampledV[Y_resampledV == 0] / W_resampledV[Y_resampledV == 0].sum())*W_resampledV.shape[0]/2\n",
    "            W_resampledV[Y_resampledV == 1] = ( W_resampledV[Y_resampledV == 1] / W_resampledV[Y_resampledV == 1].sum())*W_resampledV.shape[0]/2\n",
    "\n",
    "\n",
    "            # Logistic Regression\n",
    "            clf = LogisticRegression()\n",
    "\n",
    "            clf.fit(X_resampled,\n",
    "                    Y_resampled, \n",
    "                    sample_weight= W_resampled)\n",
    "            \n",
    "            \n",
    "            Y_pred = clf.predict_proba(X_resampledV)\n",
    "            y_scores = Y_pred[:, 1]\n",
    "\n",
    "            fpr, tpr, _ = roc_curve(Y_resampledV, y_scores,sample_weight=W_resampledV)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "\n",
    "        aucs_random_val.append(np.mean(aucs))\n",
    "        error_random_val.append(np.std(aucs))\n",
    "\n",
    "    plt.errorbar([1,2, 3, 4, 5],aucs_random_val, yerr=error_random_val, label=f'{2*centrus} datapoints')\n",
    "plt.plot([1,2, 3, 4, 5], aucs_original, label='Original dataset', linestyle='--')\n",
    "\n",
    "plt.grid(True)    \n",
    "plt.rcParams['font.size'] = MEDIUM_SIZE\n",
    "plt.xlabel('Number of features', fontsize = MEDIUM_SIZE)\n",
    "plt.ylabel('AUC', fontsize = MEDIUM_SIZE)\n",
    "plt.xticks(fontsize = TICK_SIZE)\n",
    "plt.yticks(fontsize = TICK_SIZE)\n",
    "plt.legend(fontsize = MEDIUM_SIZE)\n",
    "plt.savefig(\"plot_roc_1g_random_val.pdf\",  format='pdf')\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a79168954febbe386e45b0144a4e83e0e98e6df6e455d94ba470647bc8efc3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
